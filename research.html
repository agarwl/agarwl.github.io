---
layout: layout
title: "Research Overview"
---

<center>  
  Here's my <a href="https://scholar.google.ca/citations?user=aH8AJu4AAAAJ&hl=en">Google Scholar</a>. Here's <a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">how one should do research</a> according to <a href="https://en.wikipedia.org/wiki/Richard_Hamming">Richard Hamming</a>. <br>
  Jump to <a href='#preprints'>preprints</a> or <a href='#publications'>publications.</a>
</center>

<h2 id='publications' class="page-heading">Publications</h1>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/rliable_atari100k.jpeg" width="150">
      <table>
        <tr>          
          <td><a href="http://agarwl.github.io/rliable">Website</a></td>
          <td><a href="https://twitter.com/agarwl_/status/1432800830621687817?s=20">#TweePrint</a></td>
          <td><a href="https://arxiv.org/abs/2108.13264">ArXiv</a></td>
          <td><a href="https://github.com/google-research/rliable">Library</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Deep Reinforcement Learning at the Edge of the Statistical Precipice</b>
      <p> <b>Rishabh Agarwal</b>, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G. Bellemare <br />
      NeurIPS 2021 (<span style='color:red'>Oral</span>) (<span style='color:red'>Spotlight</span>)</p>
      </p>
    <p> Our findings call for a change in how we evaluate performance on deep RL benchmarks, for which we present more reliable protocols and an 
      <a href="https://github.com/google-research/rliable"> open-source library </a>, easily applicable with *even a handful of runs*, to prevent unreliable results 
        from stagnating the field.
    </p>
    </div>
  </div>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/nam.jpeg" width="150">
      <table>
        <tr>          
          <td><a href="http://neural-additive-models.github.io/">Website</a></td>
          <td><a href="https://twitter.com/nickfrosst/status/1255889440083447810?s=20">#TweePrint</a></td>
          <td><a href="https://arxiv.org/abs/2004.13912">ArXiv</a></td>
          <td><a href="https://github.com/google-research/google-research/tree/master/neural_additive_models">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Neural Additive Models: Interpretable ML with Neural Nets</b>
      <p> <b>Rishabh Agarwal</b>, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, Geoffrey Hinton<br />
      NeurIPS 2021 (<span style='color:red'>Spotlight</span>) (<span style='color:red'>Spotlight</span>)</p>
      </p>
    <p> We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility 
      of generalized additive models. NAMs are likely to be combined with deep learning methods in ways we don't foresee. 
      This is important because one of the key drawbacks of deep learning is intelligibility. 
    </p>
    </div>
  </div>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/behavioral_similarity.png">
      <table>
        <tr>          
          <td><a href="https://agarwl.github.io/pse">Website</a></td>
          <td><a href="https://arxiv.org/abs/2101.05265">Paper</a></td>
          <td><a href="https://slideslive.com/38942373">Talk</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning </b>
      <p> <b>Rishabh Agarwal</b>, Marlos C. Machado, Pablo Samuel Castro, Marc G. Bellemare <br />
      ICLR 2021 (<span style='color:red'>Spotlight</span>) </p>
    <p> To improve generalization, we learn representations, via a contrastive loss, that puts states together with similar long-term optimal behavior. This is orthogonal to existing 
    approaches such as data augmentation. An earlier version was accepted as an oral presentation at NeurIPS 2020 Workshop on Biological and Artificial RL. </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/iup.png">
      <table>
        <tr>
          <td><a href="https://agarwl.github.io/iup">Website</a></td>
          <td><a href="https://arxiv.org/abs/2010.14498">ArXiv</a></td>
          <td><a href="https://www.youtube.com/watch?v=dgnpGl2iNw8">Talk</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning </b>
      <p> Aviral Kumar*, <b>Rishabh Agarwal*</b>, Dibya Ghosh, Sergey Levine <br /> ICLR 2021 </p>
    <p> We identify an implicit under-parameterization phenomenon in value-based deep RL methods that use bootstrapping: when value functions, 
      approximated using deep neural networks, are trained with gradient descent using iterated regression onto target values generated by 
      previous instances of the value network, more gradient updates decrease the expressivity of the current value network. </p>
    </div>
  </div>
  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/rl_unplugged.png">
      <table>
        <tr>
          <td><a href="https://arxiv.org/abs/2006.13888">ArXiv</a></td>
          <td><a href="https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> RL Unplugged: Benchmarks for Offline Reinforcement Learning </b>
      <p> Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio GÃ³mez Colmenarejo, Konrad Zolna, <b>Rishabh Agarwal</b>, 
        Josh Merel, Daniel Mankowitz, Cosmin Paduraru, Gabriel Dulac-Arnold, Jerry Li, Mohammad Norouzi, Matt Hoffman, Ofir Nachum, 
        George Tucker, Nicolas Heess, Nando de Freitas <br /> NeurIPS 2020 </p>
    <p> We propose a benchmark called RL Unplugged to evaluate and compare offline RL methods on a diverse range of domains. We provide detailed evaluation 
      protocols for each domain and provide an extensive analysis of existing methods using these protocols. We hope that our suite of benchmarks will 
      increase the reproducibility in offline RL and make it possible to study challenging tasks with a limited computational budget, thus making RL research
      both more systematic and more accessible across the community.
    </p>
    </div>
  </div>

 <div class="divider"></div>

  
 <div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/OFFLINE_RL.gif">
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/1907.04543">ArXiv</a></td>
        <td><a href="https://offline-rl.github.io">Website</a></td>
        <td><a href="https://slideslive.com/38922701/contributed-talk-striving-for-simplicity-in-offpolicy-deep-reinforcement-learning">Talk</a></td>
      </tr>
    </table>
  </div>

    <div class="six columns">

      <b> An Optimistic Perspective on Offline Reinforcement Learning</b>
      <p> <b>Rishabh Agarwal</b>, Dale Schuurmans, Mohammad Norouzi <br />
      ICML 2020 (<span style='color:red'>Talk</span>)</p>
    <p> This paper presents an optimistic view that standard off-policy algorithms perform quite well in the fully
      off-policy / offline deep RL setting with large and diverse datasets. Furthermore, we find that simple algorithms inspired
      from supervised learning (<i>e.g.</i>, a random ensemble of <i>Q</i>-estimates) perform comparably to recent off-policy
      algorithms in the offline setting on Atari 2600 games. A previous version was titled "Striving for Simplicity in Off-Policy Deep Reinforcement Learning"
      and presented as a contributed talk at NeurIPS 2019 DRL workshop.
    </p>
    </div>
 </div>

<div class="divider"></div>

 <div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/replay.jpg">
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/2007.06700">ArXiv</a></td>
        <td><a href="https://github.com/google-research/google-research/tree/master/experience_replay">Code</a></td>
      </tr>
    </table>
  </div>

    <div class="six columns">

      <b> Revisiting Fundamentals of Experience Replay</b>
      <p> William Fedus, Prajit Ramachandran, <b>Rishabh Agarwal</b>, Yoshua Bengio, Hugo Larochelle, Mark Rowland, Will Dabney <br />
      ICML 2020 (<span style='color:red'>Talk</span>)</p>
    <p> Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. 
      We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: 
      the replay capacity and the ratio of learning updates to experience collected (replay ratio).
    </p>
    </div>
 </div>

<div class="divider"></div>

<div class="row">
  <div class="six columns">
<img style="margin-top:0em" src="/images/research/merl.png">
<table>
  <tr>
    <td><a href="https://arxiv.org/abs/1902.07198">ArXiv</a></td>
    <td><a href="https://agarwl.github.io/merl/">Website</a></td>
    <td><a href="https://www.youtube.com/watch?v=_IXj6kXPdq8&feature=youtu.be">Talk</a></td>
    <!-- <td><a href="https://github.com/google-research/google-research/tree/master/meta_reward_learning">Code</a></td> -->
  </tr>
</table>
  </div>

  <div class="six columns">

    <b> Learning to Generalize from Sparse and Underspecified Rewards</b>
    <p><b>Rishabh Agarwal</b>, Chen Liang, Dale Schuurmans, Mohammad Norouzi <br />
    ICML 2019 (<span style='color:red'>Short Oral</span>).
  </p>
 <!-- <p>Reinforcement learning (RL) has enabled remarkable success in addressing challenging tasks such as playing games such as Atari and Go, continuous control, and robotic learning. -->
  <p> Many real-world problems involve sparse and underspecified rewards, requiring a learning agent to generalize from limited feedback. In this work, we propose an effective exploration strategy for tackling sparse rewards and Meta Reward Learning to deal with underspecified rewards. These approaches provide substantial gains in language understanding tasks such as instruction following and semantic parsing.</p>
    </p>
  </div>
</div>

<div class="divider"></div>

<h2 id='preprints' class="page-heading">Preprints</h1>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/nam.jpeg" width="150">
      <table>
        <tr>          
          <td><a href="http://neural-additive-models.github.io/">Website</a></td>
          <td><a href="https://twitter.com/nickfrosst/status/1255889440083447810?s=20">#TweePrint</a></td>
          <td><a href="https://arxiv.org/abs/2004.13912">ArXiv</a></td>
          <td><a href="https://github.com/google-research/google-research/tree/master/neural_additive_models">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Neural Additive Models: Interpretable ML with Neural Nets</b>
      <p> <b>Rishabh Agarwal</b>, Nicholas Frosst, Xuezhou Zhang, Rich Caruana, Geoffrey Hinton<br />
      ICML 2020 Workshop on Human Interpretability in ML (<span style='color:red'>Spotlight</span>)</p>
      </p>
    <p> We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility 
      of generalized additive models. NAMs are likely to be combined with deep learning methods in ways we don't foresee. 
      This is important because one of the key drawbacks of deep learning is intelligibility. 
    </p>
    </div>
  </div>

</div>
