---
layout: layout
title: "Research Overview"
---

<center>  
  Here's my <a href="https://scholar.google.ca/citations?user=aH8AJu4AAAAJ&hl=en">Google Scholar</a>. Here's <a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">how one should do research</a> according to <a href="https://en.wikipedia.org/wiki/Richard_Hamming">Richard Hamming</a>. <br>
  Jump to <a href='#preprints'>preprints</a> or <a href='#publications'> selected publications.</a>
</center>

<h2 id='preprints' class="page-heading">Preprints</h1>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/gkd.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2306.13649">ArXiv</a></td>
          <td><a href="https://twitter.com/agarwl_/status/1674516600551088135?s=20">TweetPrint</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Generalized Knowledge Distillation for Auto-regressive Sequence Models </b>
      <p> <b>Rishabh Agarwal*</b>, Nino Vieillard*, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, Olivier Bachem <br/> </p>
    <p> GKD tackles distribution-mismatch and model under-specification in distilling autoregressive models, and outperforms commonly-used approaches on distilling LLMs for 
      summarization, translation and reasoning tasks. </p>
    </div>
  </div>

<div class="divider"></div>


<h2 id='publications' class="page-heading"> Selected Publications</h1>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/bbf.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2305.19452">ArXiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Bigger, Better, Faster: Human-level Atari with human-level efficiency </b>
      <p> Max Schwarzer, Johan Obando-Ceron, Aaron Courville, Marc Bellemare, Pablo Samuel Castro*, <b>Rishabh Agarwal*</b> <br />
      ICML 2023 </p>
    <p> With scaling compute and model size along with appropriate design choices, value-based methods achieve super-human performance in the Atari 100K, while being 4x compute efficient than SOTA. 
    </p>
    </div>
  </div>

  <div class="divider"></div>



  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/scaled_ql.gif">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2211.15144">ArXiv</a></td>
          <td><a href="https://ai.googleblog.com/2023/02/pre-training-generalist-agents-using.html"> Blog </a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes </b>
      <p> Aviral Kumar, <b>Rishabh Agarwal</b>, Xinyang Geng, George Tucker, Sergey Levine <br />
      ICLR 2023 (<span style='color:red'>Top 5%</span>), NeurIPS 2022 DRL Workshop <span style='color:red'>Best paper Runner-up</span> </p>
    <p> With appropriate design choices, offline Q-learning exhibit strong performance that scales with model capacity. The secret ingredients 
      were training on a large and diverse offline dataset with ResNets, distributional C51 backups and feature normalization (that is make RL training look more like SL).  
    </p>
    </div>
  </div>

  <div class="divider"></div>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/RRL.gif">
<!--       <img style="margin-top:0em" src="/images/research/image_reincarnation.png"> -->
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2206.01626">ArXiv</a></td>
          <td><a href="https://agarwl.github.io/reincarnating_rl"> Website </a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Beyond Tabula Rasa: Reincarnating Reinforcement Learning </b>
      <p> <b>Rishabh Agarwal</b>, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G. Bellemare <br />
      NeurIPS 2022 </p>
    <p> This work proposes an alternative research workflow to tabula rasa RL, where prior computational work (e.g., learned policies) is transferred from agent to another. 
    </p>
    </div>
  </div>

  <div class="divider"></div>


  <div class="row">
    <div class="six columns">
<!--       <img style="margin-top:0em" src="/images/research/rliable_atari100k.jpeg" width="150"> -->
     <iframe width="500" height="290" src="https://www.youtube.com/embed/XSY9JwqD-bw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <table>
        <tr>          
          <td><a href="http://agarwl.github.io/rliable">Website</a></td>
          <td><a href="https://twitter.com/agarwl_/status/1432800830621687817?s=20">#TweePrint</a></td>
          <td><a href="https://arxiv.org/abs/2108.13264">ArXiv</a></td>
          <td><a href="https://github.com/google-research/rliable">Library</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Deep RL at the Edge of the Statistical Precipice</b>
      <p> <b>Rishabh Agarwal</b>, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G. Bellemare <br />
      NeurIPS 2021 (<span style='color:red'>Outstanding Paper Award</span>) </p>
      </p>
    <p> Our findings call for a change in how we evaluate performance on deep RL benchmarks, for which we present more reliable protocols and an 
      <a href="https://github.com/google-research/rliable"> open-source library </a>, easily applicable with *even a handful of runs*, to prevent unreliable results 
        from stagnating the field.
    </p>
    </div>
  </div>

  <div class="divider"></div>

<!-- 
<!--   <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/nam.jpeg" width="150">
      <table>
        <tr>          
          <td><a href="http://neural-additive-models.github.io/">Website</a></td>
          <td><a href="https://twitter.com/nickfrosst/status/1255889440083447810?s=20">#TweePrint</a></td>
          <td><a href="https://arxiv.org/abs/2004.13912">ArXiv</a></td>
          <td><a href="https://github.com/google-research/google-research/tree/master/neural_additive_models">Code</a></td>
        </tr>
      </table>
    </div> -->

<!--     <div class="six columns">

      <b> Neural Additive Models: Interpretable ML with Neural Nets</b>
      <p> <b>Rishabh Agarwal</b>, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, Geoffrey Hinton<br />
      NeurIPS 2021 (<span style='color:red'>Spotlight</span>) </p>
      </p>
    <p> We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility 
      of generalized additive models. NAMs are likely to be combined with deep learning methods in ways we don't foresee. 
      This is important because one of the key drawbacks of deep learning is intelligibility. 
    </p>
    </div>  -->
<!--   </div>

  <div class="divider"></div>
 -->

  <div class="row">
    <div class="six columns">
<!--       <img style="margin-top:0em" src="/images/research/behavioral_similarity.png"> -->
      <div id="presentation-embed-38953630"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
          embed = new SlidesLiveEmbed('presentation-embed-38953630', {
              presentationId: '38953630',
              autoPlay: false, // change to true to autoplay the embedded presentation
              verticalEnabled: true
          });
      </script>
      <table>
        <tr>          
          <td><a href="https://agarwl.github.io/pse">Website</a></td>
          <td><a href="https://arxiv.org/abs/2101.05265">Paper</a></td>
          <td><a href="https://slideslive.com/38942373">Talk</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning </b>
      <p> <b>Rishabh Agarwal</b>, Marlos C. Machado, Pablo Samuel Castro, Marc G. Bellemare <br />
      ICLR 2021 (<span style='color:red'>Spotlight</span>) </p>
    <p> To improve generalization, we learn representations, via a contrastive loss, that puts states together with similar long-term optimal behavior. This is orthogonal to existing 
    approaches such as data augmentation. An earlier version was accepted as an oral presentation at NeurIPS 2020 Workshop on Biological and Artificial RL. </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/iup.png">
      <table>
        <tr>
          <td><a href="https://agarwl.github.io/iup">Website</a></td>
          <td><a href="https://arxiv.org/abs/2010.14498">ArXiv</a></td>
          <td><a href="https://www.youtube.com/watch?v=dgnpGl2iNw8">Talk</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning </b>
      <p> Aviral Kumar*, <b>Rishabh Agarwal*</b>, Dibya Ghosh, Sergey Levine <br /> ICLR 2021 </p>
    <p> We identify an implicit under-parameterization phenomenon in value-based deep RL methods that use bootstrapping: when value functions, 
      approximated using deep neural networks, are trained with gradient descent using iterated regression onto target values generated by 
      previous instances of the value network, more gradient updates decrease the expressivity of the current value network. </p>
    </div>
  </div>
  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/rl_unplugged.png">
      <table>
        <tr>
          <td><a href="https://arxiv.org/abs/2006.13888">ArXiv</a></td>
          <td><a href="https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> RL Unplugged: Benchmarks for Offline Reinforcement Learning </b>
      <p> Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio GÃ³mez Colmenarejo, Konrad Zolna, <b>Rishabh Agarwal</b>, 
        Josh Merel, Daniel Mankowitz, Cosmin Paduraru, Gabriel Dulac-Arnold, Jerry Li, Mohammad Norouzi, Matt Hoffman, Ofir Nachum, 
        George Tucker, Nicolas Heess, Nando de Freitas <br /> NeurIPS 2020 </p>
    <p> We propose a benchmark called RL Unplugged to evaluate and compare offline RL methods on a diverse range of domains. We provide detailed evaluation 
      protocols for each domain and provide an extensive analysis of existing methods using these protocols. We hope that our suite of benchmarks will 
      increase the reproducibility in offline RL and make it possible to study challenging tasks with a limited computational budget, thus making RL research
      both more systematic and more accessible across the community.
    </p>
    </div>
  </div>

 <div class="divider"></div>

  
 <div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/OFFLINE_RL.gif">
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/1907.04543">ArXiv</a></td>
        <td><a href="https://offline-rl.github.io">Website</a></td>
        <td><a href="https://slideslive.com/38922701/contributed-talk-striving-for-simplicity-in-offpolicy-deep-reinforcement-learning">Talk</a></td>
      </tr>
    </table>
  </div>

    <div class="six columns">

      <b> An Optimistic Perspective on Offline Reinforcement Learning</b>
      <p> <b>Rishabh Agarwal</b>, Dale Schuurmans, Mohammad Norouzi <br />
      ICML 2020 (<span style='color:red'>Talk</span>)</p>
    <p> This paper popularized offline RL and showed that standard off-policy algorithms perform quite well in the fully
      off-policy / offline deep RL setting with large and diverse datasets. A previous version was titled "Striving for Simplicity in Off-Policy Deep Reinforcement Learning"
      and presented as a contributed talk at NeurIPS 2019 DRL workshop.
    </p>
    </div>
 </div>

<!-- <div class="divider"></div> -->

<!--  <div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/replay.jpg">
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/2007.06700">ArXiv</a></td>
        <td><a href="https://github.com/google-research/google-research/tree/master/experience_replay">Code</a></td>
      </tr>
    </table>
  </div>

    <div class="six columns">

      <b> Revisiting Fundamentals of Experience Replay</b>
      <p> William Fedus, Prajit Ramachandran, <b>Rishabh Agarwal</b>, Yoshua Bengio, Hugo Larochelle, Mark Rowland, Will Dabney <br />
      ICML 2020 (<span style='color:red'>Talk</span>)</p>
    <p> Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. 
      We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: 
      the replay capacity and the ratio of learning updates to experience collected (replay ratio).
    </p>
    </div>
 </div> -->

<!-- <div class="divider"></div> -->

<!-- <div class="row">
<!--   <div class="six columns">
<img style="margin-top:0em" src="/images/research/merl.png">
<table>
  <tr>
    <td><a href="https://arxiv.org/abs/1902.07198">ArXiv</a></td>
    <td><a href="https://agarwl.github.io/merl/">Website</a></td>
    <td><a href="https://www.youtube.com/watch?v=_IXj6kXPdq8&feature=youtu.be">Talk</a></td>
    <!-- <td><a href="https://github.com/google-research/google-research/tree/master/meta_reward_learning">Code</a></td> -->
<!--   </tr>
</table>
  </div> --> 

<!--   <div class="six columns">

    <b> Learning to Generalize from Sparse and Underspecified Rewards</b>
    <p><b>Rishabh Agarwal</b>, Chen Liang, Dale Schuurmans, Mohammad Norouzi <br />
    ICML 2019 (<span style='color:red'>Short Talk</span>).
  </p>
 <!-- <p>Reinforcement learning (RL) has enabled remarkable success in addressing challenging tasks such as playing games such as Atari and Go, continuous control, and robotic learning. -->
<!--   <p> Many real-world problems involve sparse and underspecified rewards, requiring a learning agent to generalize from limited feedback. In this work, we propose an effective exploration strategy for tackling sparse rewards and Meta Reward Learning to deal with underspecified rewards. These approaches provide substantial gains in language understanding tasks such as instruction following and semantic parsing.</p>
    </p> -->
<!--   </div> -->
<!-- </div> --> 





</div>
