---
layout: layout
title: "Research Overview"
---

<center>  
  Here's my <a href="https://scholar.google.ca/citations?user=aH8AJu4AAAAJ&hl=en">Google Scholar</a>. Here's <a href="https://fs.blog/great-talks/richard-hamming-your-research/">how one should do research</a> according to <a href="https://en.wikipedia.org/wiki/Richard_Hamming">Richard Hamming</a>. <br>
  Jump to <a href='#preprints'>preprints</a> or <a href='#publications'> selected publications.</a>
</center>

<h2 id='preprints' class="page-heading">Preprints</h1>



  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/compgsm.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2410.01748">ArXiv</a></td>
          <td><a href="https://x.com/arianTBD/status/1841875515860517130">TweetPrint</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">
      <b> Not All LLM Reasoners are Created Equal </b>
      <p> Arian Hosseini, Alessandro Sordoni, Daniel Toyama, Aaron Courville, <b>Rishabh Agarwal</b> </p>
      <p> LLMs, especially smaller and cost-efficient LLMs, exhibit systematic differences in their reasoning abilities, despite what their performance on standard math benchmarks indicates. </p>
    </div>
  </div>


<div class="divider"></div>

<h2 id='publications' class="page-heading"> Selected Publications</h1>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/genrm.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2408.15240">ArXiv</a></td>
          <td><a href="https://twitter.com/agarwl_/status/1842609385287348712">TweetPrint</a></td>
          <td><a href="https://sites.google.com/corp/view/generative-reward-models">Website</a></td>
          <td><a href="https://github.com/genrm-star/genrm-critiques/">Data</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">
      <b> Generative Verifiers: Reward Modeling as Next-Token Prediction </b>
      <p> Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, <b>Rishabh Agarwal</b>  <br/> ICLR 2025 </p>
      <p> Training generative reward models (GenRM) using next-token prediction, jointly on verification and solution generation. Such generative verifiers can use chain-of-thought (CoT) reasoning, 
      and additional test-time compute via majority voting for better verification. Generative CoT verifiers trained on GSM generalizes to much harder problems in MATH! </p>
    </div>
  </div>

<div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/genrm.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2410.18252">ArXiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">
      <b> Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models </b>
      <p> Michael Noukhovitch, Shengyi Huang, ..., <b>Rishabh Agarwal</b>, Aaron Courville  <br/> ICLR 2025 </p>
      <p> Inspired by classical deep RL, we propose separating generation and learning in RLxF. 
        This enables asynchronous generation of new samples while simultaneously training on old samples, leading to faster training and more compute-optimal scaling. </p>
    </div>
  </div>

<div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/spec_kd.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2410.11325">ArXiv</a></td>
          <td><a href="https://x.com/WendaXu2/status/1847311997547037023">TweetPrint</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">
      <b> Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling </b>
      <p> Wenda Xu, ..., <b>Rishabh Agarawl^</b>, Chen-Yu Lee^, Tomas Pfister <br/> ICLR 2025 </p>
      <p> An improvement to on-policy distillation where the student proposes tokens, but the teacher replaces tokens outside its top-k tokens to avoid low quality samples. </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/compute_optimal_star.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2409.12917">ArXiv</a></td>
          <td><a href="https://x.com/hbXNov/status/1829332120696762644">TweetPrint</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">
      <b> Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling </b>
      <p> Hritik Bansal, Arian Hosseini, <b>Rishabh Agarwal</b>, Vinh Tran, and Mehran Kazemi <br/> ICLR 2025 </p>
      <p> Under a fixed inference compute or cost budget, fine-tuning on data generated from a smaller but weaker LLM outperform those trained on stronger but larger models. </p>
    </div>
  </div>

<div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/self_correct.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2409.12917">ArXiv</a></td>
          <td><a href="https://x.com/aviral_kumar2/status/1843414539943194945">TweetPrint</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">
      <b> Training Language Models to Self-Correct via Reinforcement Learning </b>
      <p> A Kumar*, V Zhuang*, <b>Rishabh Agarwal*</b>, Y Su* et al. <br/> ICLR 2025 </p>
      <p> A multi-turn online RL approach that teaches an LLM to self-correct using entirely self-generated data. </p>
    </div>
  </div>

<div class="divider"></div>



<div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/many_shot.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2404.11018">ArXiv</a></td>
          <td><a href="https://x.com/arianTBD/status/1757121513621229829?s=20">TweetPrint</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">
      <b> Many-Shot In-Context Learning </b>
      <p> <b>Rishabh Agarwal*</b>, Avi Singh*, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan et al. <br/> NeurIPS 2024 <span style='color:red'>(Spotlight), <span style='color:red'>Oral</span>@ICML Long-Context Workshop </p>
    <p> Explores ICL with hundreds or thousands of examples. Unlike few-shot learning, many-shot learning is 
      effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. </p>
    </div>
  </div>

<div class="divider"></div>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/stop_regressing.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2403.03950">ArXiv</a></td>
          <td><a href="https://x.com/JesseFarebro/status/1765762861509345578">TweetPrint</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Stop Regressing: Training Value Functions via Classification for Scalable Deep RL </b>
      <p> Jesse Farebrother*, Jordi Orbay, ..., Pablo Castro, Aviral Kumar, <b>Rishabh Agarwal*</b> <br/> ICML 2024 <span style='color:red'>(Oral) </p>
    <p> Training value functions with categorical cross-entropy can substantially improve the scalability and generalizability of deep RL at little-to-no cost. </p>
    </div>
  </div>

  <div class="divider"></div>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/vstar_math.jpeg">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2402.06457">ArXiv</a></td>
          <td><a href="https://x.com/arianTBD/status/1757121513621229829?s=20">TweetPrint</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> V-STaR: Training Verifiers for Self-Taught Reasoners </b>
      <p> Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, <b>Rishabh Agarwal</b> <br/> CoLM 2024 </p>
    <p> Self-improvement approaches, such as ReST^EM and STaR, discard all the LLM-generated incorrect solutions during training. V-STaR 
      augments such approaches by training a verifier using both correct and incorrect solutions, used at test-time for re-ranking LLM generations. </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/rest_math.jpeg">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2312.06585">ArXiv</a></td>
          <td><a href="https://x.com/avisingh599/status/1734603680933192089?s=20">TweetPrint</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models </b>
      <p> <b> Avi Singh*, JD Coreyes*, Rishabh Agarwal*</b> et al. <br/> TMLR 2024 </p>
    <p> We explore whether we can go beyond human data on tasks where we have access to scalar feedback, finding that 
      a simple self-training method based on expectation-maximization can substantially reduce dependence on human-generated data. </p>
    </div>
  </div>

  <div class="divider"></div>



  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/gkd.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2306.13649">ArXiv</a></td>
          <td><a href="https://twitter.com/agarwl_/status/1674516600551088135?s=20">TweetPrint</a></td>
          <td><a href="https://huggingface.co/docs/trl/en/gkd_trainer">Code</a></td>
          
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes </b>
      <p> <b>Rishabh Agarwal*</b>, Nino Vieillard*, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, Olivier Bachem <br/> ICLR 2024 </p>
    <p> GKD tackles distribution-mismatch in distilling autoregressive models, and outperforms commonly-used approaches on distilling LLMs for 
      summarization, translation and reasoning tasks. <span style='color:red'> Used for post-training distillation for Gemma-2. </span> </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/bbf.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2305.19452">ArXiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Bigger, Better, Faster: Human-level Atari with human-level efficiency </b>
      <p> Max Schwarzer, Johan Obando-Ceron, Aaron Courville, Marc Bellemare, Pablo Samuel Castro*, <b>Rishabh Agarwal*</b> <br />
      ICML 2023 </p>
    <p> With scaling compute and model size along with appropriate design choices, value-based methods achieve super-human performance in the Atari 100K, while being 4x compute efficient than SOTA. 
    </p>
    </div>
  </div>

  <div class="divider"></div>



  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/scaled_ql.gif">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2211.15144">ArXiv</a></td>
          <td><a href="https://ai.googleblog.com/2023/02/pre-training-generalist-agents-using.html"> Blog </a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes </b>
      <p> Aviral Kumar, <b>Rishabh Agarwal</b>, Xinyang Geng, George Tucker, Sergey Levine <br />
      ICLR 2023 (<span style='color:red'>Top 5%</span>), NeurIPS 2022 DRL Workshop <span style='color:red'>Best paper Runner-up</span> </p>
    <p> With appropriate design choices, offline Q-learning exhibit strong performance that scales with model capacity. The secret ingredients 
      were training on a large and diverse offline dataset with ResNets, distributional C51 backups and feature normalization (that is make RL training look more like SL).  
    </p>
    </div>
  </div>

  <div class="divider"></div>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/RRL.gif">
<!--       <img style="margin-top:0em" src="/images/research/image_reincarnation.png"> -->
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2206.01626">ArXiv</a></td>
          <td><a href="https://agarwl.github.io/reincarnating_rl"> Website </a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Beyond Tabula Rasa: Reincarnating Reinforcement Learning </b>
      <p> <b>Rishabh Agarwal</b>, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G. Bellemare <br />
      NeurIPS 2022 </p>
    <p> This work proposes an alternative research workflow to tabula rasa RL, where prior computational work (e.g., learned policies) is transferred from agent to another. 
    </p>
    </div>
  </div>

  <div class="divider"></div>


  <div class="row">
    <div class="six columns">
<!--       <img style="margin-top:0em" src="/images/research/rliable_atari100k.jpeg" width="150"> -->
     <iframe width="500" height="290" src="https://www.youtube.com/embed/XSY9JwqD-bw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <table>
        <tr>          
          <td><a href="http://agarwl.github.io/rliable">Website</a></td>
          <td><a href="https://twitter.com/agarwl_/status/1432800830621687817?s=20">#TweePrint</a></td>
          <td><a href="https://arxiv.org/abs/2108.13264">ArXiv</a></td>
          <td><a href="https://github.com/google-research/rliable">Library</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Deep RL at the Edge of the Statistical Precipice</b>
      <p> <b>Rishabh Agarwal</b>, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G. Bellemare <br />
      NeurIPS 2021 (<span style='color:red'>Outstanding Paper Award</span>) </p>
      </p>
    <p> Our findings call for a change in how we evaluate performance on deep RL benchmarks, for which we present more reliable protocols and an 
      <a href="https://github.com/google-research/rliable"> open-source library </a>, easily applicable with *even a handful of runs*, to prevent unreliable results 
        from stagnating the field.
    </p>
    </div>
  </div>

  <div class="divider"></div>

<!-- 
<!--   <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/nam.jpeg" width="150">
      <table>
        <tr>          
          <td><a href="http://neural-additive-models.github.io/">Website</a></td>
          <td><a href="https://twitter.com/nickfrosst/status/1255889440083447810?s=20">#TweePrint</a></td>
          <td><a href="https://arxiv.org/abs/2004.13912">ArXiv</a></td>
          <td><a href="https://github.com/google-research/google-research/tree/master/neural_additive_models">Code</a></td>
        </tr>
      </table>
    </div> -->

<!--     <div class="six columns">

      <b> Neural Additive Models: Interpretable ML with Neural Nets</b>
      <p> <b>Rishabh Agarwal</b>, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, Geoffrey Hinton<br />
      NeurIPS 2021 (<span style='color:red'>Spotlight</span>) </p>
      </p>
    <p> We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility 
      of generalized additive models. NAMs are likely to be combined with deep learning methods in ways we don't foresee. 
      This is important because one of the key drawbacks of deep learning is intelligibility. 
    </p>
    </div>  -->
<!--   </div>

  <div class="divider"></div>
 -->

  <div class="row">
    <div class="six columns">
<!--       <img style="margin-top:0em" src="/images/research/behavioral_similarity.png"> -->
      <div id="presentation-embed-38953630"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
          embed = new SlidesLiveEmbed('presentation-embed-38953630', {
              presentationId: '38953630',
              autoPlay: false, // change to true to autoplay the embedded presentation
              verticalEnabled: true
          });
      </script>
      <table>
        <tr>          
          <td><a href="https://agarwl.github.io/pse">Website</a></td>
          <td><a href="https://arxiv.org/abs/2101.05265">Paper</a></td>
          <td><a href="https://slideslive.com/38942373">Talk</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning </b>
      <p> <b>Rishabh Agarwal</b>, Marlos C. Machado, Pablo Samuel Castro, Marc G. Bellemare <br />
      ICLR 2021 (<span style='color:red'>Spotlight</span>) </p>
    <p> To improve generalization, we learn representations, via a contrastive loss, that puts states together with similar long-term optimal behavior. This is orthogonal to existing 
    approaches such as data augmentation. An earlier version was accepted as an oral presentation at NeurIPS 2020 Workshop on Biological and Artificial RL. </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/iup.png">
      <table>
        <tr>
          <td><a href="https://agarwl.github.io/iup">Website</a></td>
          <td><a href="https://arxiv.org/abs/2010.14498">ArXiv</a></td>
          <td><a href="https://www.youtube.com/watch?v=dgnpGl2iNw8">Talk</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning </b>
      <p> Aviral Kumar*, <b>Rishabh Agarwal*</b>, Dibya Ghosh, Sergey Levine <br /> ICLR 2021 </p>
    <p> We identify an implicit under-parameterization phenomenon in value-based deep RL methods that use bootstrapping: when value functions, 
      approximated using deep neural networks, are trained with gradient descent using iterated regression onto target values generated by 
      previous instances of the value network, more gradient updates decrease the expressivity of the current value network. </p>
    </div>
  </div>
  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/rl_unplugged.png">
      <table>
        <tr>
          <td><a href="https://arxiv.org/abs/2006.13888">ArXiv</a></td>
          <td><a href="https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> RL Unplugged: Benchmarks for Offline Reinforcement Learning </b>
      <p> Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gómez Colmenarejo, Konrad Zolna, <b>Rishabh Agarwal</b>, 
        Josh Merel, Daniel Mankowitz, Cosmin Paduraru, Gabriel Dulac-Arnold, Jerry Li, Mohammad Norouzi, Matt Hoffman, Ofir Nachum, 
        George Tucker, Nicolas Heess, Nando de Freitas <br /> NeurIPS 2020 </p>
    <p> We propose a benchmark called RL Unplugged to evaluate and compare offline RL methods on a diverse range of domains. We provide detailed evaluation 
      protocols for each domain and provide an extensive analysis of existing methods using these protocols. We hope that our suite of benchmarks will 
      increase the reproducibility in offline RL and make it possible to study challenging tasks with a limited computational budget, thus making RL research
      both more systematic and more accessible across the community.
    </p>
    </div>
  </div>

 <div class="divider"></div>

  
 <div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/OFFLINE_RL.gif">
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/1907.04543">ArXiv</a></td>
        <td><a href="https://offline-rl.github.io">Website</a></td>
        <td><a href="https://slideslive.com/38922701/contributed-talk-striving-for-simplicity-in-offpolicy-deep-reinforcement-learning">Talk</a></td>
      </tr>
    </table>
  </div>

    <div class="six columns">

      <b> An Optimistic Perspective on Offline Reinforcement Learning</b>
      <p> <b>Rishabh Agarwal</b>, Dale Schuurmans, Mohammad Norouzi <br />
      ICML 2020 (<span style='color:red'>Talk</span>)</p>
    <p> This paper popularized offline RL and showed that standard off-policy algorithms perform quite well in the fully
      off-policy / offline deep RL setting with large and diverse datasets. A previous version was titled "Striving for Simplicity in Off-Policy Deep Reinforcement Learning"
      and presented as a contributed talk at NeurIPS 2019 DRL workshop.
    </p>
    </div>
 </div>

<!-- <div class="divider"></div> -->

<!--  <div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/replay.jpg">
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/2007.06700">ArXiv</a></td>
        <td><a href="https://github.com/google-research/google-research/tree/master/experience_replay">Code</a></td>
      </tr>
    </table>
  </div>

    <div class="six columns">

      <b> Revisiting Fundamentals of Experience Replay</b>
      <p> William Fedus, Prajit Ramachandran, <b>Rishabh Agarwal</b>, Yoshua Bengio, Hugo Larochelle, Mark Rowland, Will Dabney <br />
      ICML 2020 (<span style='color:red'>Talk</span>)</p>
    <p> Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. 
      We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: 
      the replay capacity and the ratio of learning updates to experience collected (replay ratio).
    </p>
    </div>
 </div> -->

<!-- <div class="divider"></div> -->

<!-- <div class="row">
<!--   <div class="six columns">
<img style="margin-top:0em" src="/images/research/merl.png">
<table>
  <tr>
    <td><a href="https://arxiv.org/abs/1902.07198">ArXiv</a></td>
    <td><a href="https://agarwl.github.io/merl/">Website</a></td>
    <td><a href="https://www.youtube.com/watch?v=_IXj6kXPdq8&feature=youtu.be">Talk</a></td>
    <!-- <td><a href="https://github.com/google-research/google-research/tree/master/meta_reward_learning">Code</a></td> -->
<!--   </tr>
</table>
  </div> --> 

<!--   <div class="six columns">

    <b> Learning to Generalize from Sparse and Underspecified Rewards</b>
    <p><b>Rishabh Agarwal</b>, Chen Liang, Dale Schuurmans, Mohammad Norouzi <br />
    ICML 2019 (<span style='color:red'>Short Talk</span>).
  </p>
 <!-- <p>Reinforcement learning (RL) has enabled remarkable success in addressing challenging tasks such as playing games such as Atari and Go, continuous control, and robotic learning. -->
<!--   <p> Many real-world problems involve sparse and underspecified rewards, requiring a learning agent to generalize from limited feedback. In this work, we propose an effective exploration strategy for tackling sparse rewards and Meta Reward Learning to deal with underspecified rewards. These approaches provide substantial gains in language understanding tasks such as instruction following and semantic parsing.</p>
    </p> -->
<!--   </div> -->
<!-- </div> --> 





</div>
